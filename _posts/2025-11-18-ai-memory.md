# Introduction to Memory
## Agent Architecture
![](https://intranetproxy.alipay.com/skylark/lark/0/2025/png/121056529/1762676410529-58151223-212e-4f18-8fa9-d3de6788c8f9.png)

(From The Rise and Potential of Large Language Model Based Agents: A Survey):

1. Perception: Information input.
2. Brain-Decision Making: Autonomous decision-making and planning, executing more complex tasks.
3. Brain-Memory & Knowledge: Memory capability, storing Agent's knowledge and skills.
4. Action: Interacting with the external world, enabling Agents to autonomously complete more complex tasks through actions and perception.

## The Role of Memory for Agents
1. <font style="color:rgb(37, 39, 42);">Enable Agents to have continuous learning capabilities</font>
    - <font style="color:rgb(37, 39, 42);">Summarize experiences from past interactions and learn from mistakes to improve task performance.</font>
2. <font style="color:rgb(37, 39, 42);">Enable Agents to maintain conversational coherence and action consistency:</font>
    - <font style="color:rgb(37, 39, 42);">Possess longer-range context management capabilities, maintaining consistent context in long conversations to ensure coherence.</font>
    - <font style="color:rgb(37, 39, 42);">Avoid establishing facts that contradict previous ones, maintaining action consistency.</font>
3. <font style="color:rgb(37, 39, 42);">Enable Agents to provide personalized services and user experiences</font>

![](https://intranetproxy.alipay.com/skylark/lark/0/2025/png/121056529/1762676911596-f89ff7b9-3e1a-4413-a473-c6dd62122a8c.png)

# Memory VS RAG
+ **<font style="color:rgb(17, 17, 51);">AI Memory</font>**<font style="color:rgb(17, 17, 51);"> is more like the model's "personal experience" or "conversation history", focusing on</font>**<font style="color:rgb(17, 17, 51);"> continuity of internal state and personalization</font>**<font style="color:rgb(17, 17, 51);">.</font>
+ **<font style="color:rgb(17, 17, 51);">RAG</font>**<font style="color:rgb(17, 17, 51);"> </font><font style="color:rgb(17, 17, 51);">is like equipping the model with a "real-time reference library", focusing on</font>**<font style="color:rgb(17, 17, 51);"> obtaining external, authoritative, and up-to-date information</font>**<font style="color:rgb(17, 17, 51);"> to improve answer accuracy and reliability.</font>

<font style="color:rgb(17, 17, 51);">Simply put, you can think of AI Memory as the model "remembering" what it has discussed with you, while RAG is the model "looking up information" when answering your questions to ensure correctness. An intelligent AI assistant will likely use both capabilities simultaneously.</font>

## Differences
| **<font style="color:rgb(17, 17, 51);">Feature</font>** | **<font style="color:rgb(17, 17, 51);">AI Memory</font>** | **<font style="color:rgb(17, 17, 51);">RAG (Retrieval-Augmented Generation)</font>** |
| :--- | :--- | :--- |
| **<font style="color:rgb(17, 17, 51);">Core Mechanism</font>** | <font style="color:rgb(17, 17, 51);">Influences future outputs by storing and retrieving the model's own</font>**<font style="color:rgb(17, 17, 51);"> interaction history or internal knowledge</font>**<font style="color:rgb(17, 17, 51);">. Memory can be short-term (e.g., conversation context) or long-term (e.g., past experiences stored in vector databases).</font> | <font style="color:rgb(17, 17, 51);">Before generating a response, retrieves relevant information from</font>**<font style="color:rgb(17, 17, 51);"> external knowledge bases</font>**<font style="color:rgb(17, 17, 51);"> (e.g., documents, databases, web pages) and uses the retrieved content as context input to the model.</font> |
| **<font style="color:rgb(17, 17, 51);">Knowledge Source</font>** | <font style="color:rgb(17, 17, 51);">Primarily from the model's</font>**<font style="color:rgb(17, 17, 51);"> interaction history</font>**<font style="color:rgb(17, 17, 51);"> with users or system-preset long-term memory.</font> | <font style="color:rgb(17, 17, 51);">From</font>**<font style="color:rgb(17, 17, 51);"> external, independent, typically structured knowledge bases</font>**<font style="color:rgb(17, 17, 51);">.</font> |
| **<font style="color:rgb(17, 17, 51);">Primary Purpose</font>** | 1. **<font style="color:rgb(17, 17, 51);">Maintain context consistency</font>**<font style="color:rgb(17, 17, 51);"> (e.g., multi-turn conversations).</font><br/>2. **<font style="color:rgb(17, 17, 51);">Personalization</font>**<font style="color:rgb(17, 17, 51);"> (remembering user preferences).</font><br/>3. <font style="color:rgb(17, 17, 51);"></font>**<font style="color:rgb(17, 17, 51);">Learning and adaptation</font>**<font style="color:rgb(17, 17, 51);"> (learning from past experiences).</font> | 1. **<font style="color:rgb(17, 17, 51);">Provide latest, accurate factual information</font>**<font style="color:rgb(17, 17, 51);">.</font><br/>2. <font style="color:rgb(17, 17, 51);"> </font>**<font style="color:rgb(17, 17, 51);">Reduce hallucinations</font>**<font style="color:rgb(17, 17, 51);"> (generating based on retrieved real information).</font><br/>3. **<font style="color:rgb(17, 17, 51);">Extend model knowledge boundaries</font>**<font style="color:rgb(17, 17, 51);"> (accessing information beyond training data).</font> |
| **<font style="color:rgb(17, 17, 51);">Knowledge Updates</font>** | <font style="color:rgb(17, 17, 51);">Memory can be dynamically updated, but requires managing memory</font>**<font style="color:rgb(17, 17, 51);"> storage, retrieval, and forgetting</font>**<font style="color:rgb(17, 17, 51);">. Updates may affect model behavior.</font> | <font style="color:rgb(17, 17, 51);">Knowledge bases can be updated independently without retraining the model. Updates are</font>**<font style="color:rgb(17, 17, 51);"> decoupled</font>**<font style="color:rgb(17, 17, 51);"> from the model.</font> |
| **<font style="color:rgb(17, 17, 51);">Implementation</font>** | + **<font style="color:rgb(17, 17, 51);">Short-term memory</font>**<font style="color:rgb(17, 17, 51);">: Directly using the model's context window.</font><br/>+ **<font style="color:rgb(17, 17, 51);">Long-term memory</font>**<font style="color:rgb(17, 17, 51);">: Using vector databases to store and retrieve key information fragments.</font> | + **<font style="color:rgb(17, 17, 51);">Retriever</font>**<font style="color:rgb(17, 17, 51);"> (e.g., vector-based similarity search).</font><br/>+ **<font style="color:rgb(17, 17, 51);">Generator</font>**<font style="color:rgb(17, 17, 51);"> (large language model).</font><br/>+ **<font style="color:rgb(17, 17, 51);">Knowledge base</font>**<font style="color:rgb(17, 17, 51);"> (document collection).</font> |
| **<font style="color:rgb(17, 17, 51);">Typical Applications</font>** | <font style="color:rgb(17, 17, 51);">Chatbots maintaining conversation coherence, personalized assistants, AI agents requiring long-term learning.</font> | <font style="color:rgb(17, 17, 51);">Question-answering systems, customer service knowledge bases, fact-checking, report generation (requiring external references).</font> |


# Memory Implementation
## Mem0
![](https://intranetproxy.alipay.com/skylark/lark/0/2025/png/121056529/1758008181340-7a31ba73-2ef0-43b0-9546-7726281b4aa4.png)

### <font style="color:rgb(65, 65, 69);">Components</font>
+ <font style="color:rgb(65, 65, 69);">L</font>LM (OpenAI, Ollama, Azure OpenAI, Gemini, DeepSeek, and 18 other types)
+ <font style="color:rgb(65, 65, 69);">Vector databases</font> (Qdrant, Chroma, Pgvector, Milvus, and 19 other types)
+ <font style="color:rgb(65, 65, 69);">Graph databases</font> (Neo4j, Memgraph, Neptune Analytics, Kuzu)
+ <font style="color:rgb(65, 65, 69);">Embedding models</font> (OpenAI, Azure OpenAI, Ollama, and 10 other types)

### <font style="color:rgb(65, 65, 69);">Architecture</font>
+ <font style="color:rgb(65, 65, 69);">API interface layer</font>
+ <font style="color:rgb(65, 65, 69);">Intelligent processing layer: LLM reasoning + fact extraction + conflict resolution</font>
    - Fact extraction: Using custom prompts to extract key information from conversations
    - Conflict resolution: Intelligently determining ADD/UPDATE/DELETE/NONE operations
    - Memory optimization: Avoiding redundancy, keeping information up-to-date
+ <font style="color:rgb(65, 65, 69);">Dual storage architecture: Vector database + Graph database</font>
+ <font style="color:rgb(65, 65, 69);">Factory layer: LLM/Embedder/VectorStore Factory</font>

### <font style="color:rgb(65, 65, 69);">Process</font>
**Main Process**

![](https://intranetproxy.alipay.com/skylark/lark/__mermaid_v3/c7e974749183b4f7f08d49a828b4a811.svg)

**<font style="color:rgb(65, 65, 69);">Retrieval Process</font>**

![](https://intranetproxy.alipay.com/skylark/lark/__mermaid_v3/24193f5daddd60d9872b360f73b5fa8d.svg)

### Source Code Analysis
```python
# mem0's concurrent dual-path retrieval implementation
with concurrent.futures.ThreadPoolExecutor() as executor:
    # Path 1: Vector database semantic search
    future_memories = executor.submit(self._search_vector_store, ...)

    # Path 2: Graph database relationship search  
    future_graph_entities = executor.submit(self.graph.search, ...)

    # Get results
    vector_memories = future_memories.result()     # Retrieval results
    graph_relations = future_graph_entities.result()  # Graph path entity relationship results 

# Return separately
return {
    "results": vector_memories,    # Vector path results
    "relations": graph_relations   # Graph path entity relationship results 
}
```



```python
def search(
    self,
    query: str,
    vectors: list[float],
    limit: Optional[int] = 5,
    filters: Optional[dict] = None,
) -> List[OutputData]:
    
    ...
    
    # Filter conditions
    if filters:
        for k, v in filters.items():
            filter_conditions.append("payload->>%s = %s")
            filter_params.extend([k, str(v)])
    filter_clause = "WHERE " + " AND ".join(filter_conditions) if filter_conditions else ""

    # Execute SQL: scalar + vector
    with self._get_cursor() as cur:
        cur.execute(
            f"""
            SELECT id, vector <=> %s::vector AS distance, payload
            FROM {self.collection_name}
            {filter_clause}
            ORDER BY distance
            LIMIT %s
            """,
            (vectors, *filter_params, limit),
        )

        results = cur.fetchall()
    ...
```

```python
def search(self, query, filters, limit=100):
    ...

    # Step 1: LLM extracts entities
    entity_type_map = self._retrieve_nodes_from_data(query, filters)
    # Step 2: Graph search for related entities and relationships
    search_output = self._search_graph_db(node_list=list(entity_type_map.keys()), filters=filters)

    ...
    
    # Step 3: BM25 reranks search results
    bm25 = BM25Okapi(search_outputs_sequence)

    tokenized_query = query.split(" ")
    reranked_results = bm25.get_top_n(tokenized_query, search_outputs_sequence, n=5)

    ...

# Implementation of _search_graph_db graph search
def _search_graph_db(self, node_list, filters, limit=100):
    
    # Filter conditions
    ...
    node_props_str = ...
    
    for node in node_list:
        n_embedding = self.embedding_model.embed(node)
    
        # Node retrieval based on vector similarity
        cypher_query = f"""
        -------Match candidate nodes-------
        ---self.node_label = ":`__Entity__`" if self.config.graph_store.config.base_label else ""
        MATCH (n {self.node_label} {{{node_props_str}}})
        WHERE n.embedding IS NOT NULL
        -------Calculate cosine similarity-------
        WITH n, round(2 * vector.similarity.cosine(n.embedding, $n_embedding) - 1, 4) AS similarity // denormalize for backward compatibility
        WHERE similarity >= $threshold
        -------Find outgoing and incoming edges-------
        CALL {{
            WITH n
            MATCH (n)-[r]->(m {self.node_label} {{{node_props_str}}})
            RETURN n.name AS source, elementId(n) AS source_id, type(r) AS relationship, elementId(r) AS relation_id, m.name AS destination, elementId(m) AS destination_id
            UNION
            WITH n  
            MATCH (n)<-[r]-(m {self.node_label} {{{node_props_str}}})
            RETURN m.name AS source, elementId(m) AS source_id, type(r) AS relationship, elementId(r) AS relation_id, n.name AS destination, elementId(n) AS destination_id
        }}
        WITH distinct source, source_id, relationship, relation_id, destination, destination_id, similarity
        RETURN source, source_id, relationship, relation_id, destination, destination_id, similarity
        ORDER BY similarity DESC
        LIMIT $limit
        """
    
        params = {
            "n_embedding": n_embedding,
            "threshold": self.threshold,
            "user_id": filters["user_id"],
            "limit": limit,
        }
        
        ...
        
        ans = self.graph.query(cypher_query, params=params)
        result_relations.extend(ans)
    
    return result_relations
```

## <font style="color:rgb(62, 62, 62);">PowerMem</font>
### Performance Improvements
#### Multi-path Retrieval/Hybrid Search
**Flow Diagram**

+ Full-text + Vector + Scalar
    - Version 441 and above can directly use kernel hybrid search
+ Fusion coarse ranking (RRF) + Fine ranking (Rerank model)
    - Rerank can improve accuracy by ~6%

![](https://intranetproxy.alipay.com/skylark/lark/__mermaid_v3/0419ae711535f7dd9b2920933a467a26.svg)

**Source Code Analysis:**

```python
def _hybrid_search(self, query: str, vectors: List[List[float]], limit: int = 5, filters: Optional[Dict] = None,
                   fusion_method: str = "rrf", k: int = 60):

    # Expand single-path limit by 3x
    candidate_limit = limit * 3 if self.reranker else limit

    # Step 1: Query separately
    with ThreadPoolExecutor(max_workers=2) as executor:
        # Vector
        vector_future = executor.submit(self._vector_search, query, vectors, candidate_limit, filters)
        # Full-text
        fts_future = executor.submit(self._fulltext_search, query, candidate_limit, filters)

        vector_results = vector_future.result()
        fts_results = fts_future.result()

    # Step 2: Fuse results for coarse ranking, limit results to 3 * limit
    coarse_ranked_results = self._combine_search_results(
        vector_results, fts_results, candidate_limit, fusion_method, k
    )

    
    # Step 3: Fine ranking (using qwen3-rerank model), limit results to limit
    if self.reranker and query and coarse_ranked_results:
        final_results = self._apply_rerank(query, coarse_ranked_results, limit)
        return final_results
    else:
        return coarse_ranked_results[:limit]

# rerank
def _apply_rerank(self, query: str, candidates: List[OutputData], limit: int) -> List[OutputData]:
    
    ...

    # Call reranker to get reranked indices and scores
    reranked_indices = self.reranker.rerank(query, documents, top_n=limit)
    
    ...
    
    # Special handling: high-scoring items at head and tail, because LLMs have recency effect
    if len(final_results) > 1:
        reordered = [None] * len(final_results)
        left = 0
        right = len(final_results) - 1
        
        for i, result in enumerate(final_results):
            if i % 2 == 0:
                # Even indices go to the left side
                reordered[left] = result
                left += 1
            else:
                # Odd indices go to the right side
                reordered[right] = result
                right -= 1
        
        final_results = reordered

    return final_results
```



**<font style="color:rgb(13, 18, 57);">Recency Effect</font>** Source:

> + **<font style="color:rgb(13, 18, 57);">Paper Title</font>**<font style="color:rgb(13, 18, 57);">: </font>**<font style="color:rgb(13, 18, 57);">Lost in the Middle: How Language Models Use Long Contexts</font>**
> + **<font style="color:rgb(13, 18, 57);">Authors</font>**<font style="color:rgb(13, 18, 57);">: Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Catasta, Percy Liang</font>
> + **<font style="color:rgb(13, 18, 57);">Published</font>**<font style="color:rgb(13, 18, 57);">: 2023</font>
> + **<font style="color:rgb(13, 18, 57);">Link</font>**<font style="color:rgb(13, 18, 57);">: </font>[<font style="color:rgb(94, 92, 230);">https://arxiv.org/abs/2307.03172</font>](https://arxiv.org/abs/2307.03172)
>

![](https://intranetproxy.alipay.com/skylark/lark/0/2025/png/121056529/1763013612851-0744a233-95ee-42c7-981c-3da69854f525.png)

**<font style="color:rgb(13, 18, 57);">U-shaped Performance Curve</font>**<font style="color:rgb(13, 18, 57);">: The core finding of the paper is that when models need to retrieve information from long input texts (long contexts), their performance shows a clear "U-shaped" curve.</font>

    - <font style="color:rgb(13, 18, 57);">Information located at the</font>**<font style="color:rgb(13, 18, 57);"> beginning (Primacy) and end (Recency)</font>**<font style="color:rgb(13, 18, 57);"> of the input sequence can be most accurately recalled and used by the model.</font>
    - <font style="color:rgb(13, 18, 57);">Information located in the</font>**<font style="color:rgb(13, 18, 57);"> middle</font>**<font style="color:rgb(13, 18, 57);"> of the input sequence shows significantly decreased accuracy in recall and usage, as if "lost in the middle".</font>

**Paper Conclusions**

+ **<font style="color:rgb(13, 18, 57);">Why must System Prompt be at the front?</font>**<font style="color:rgb(13, 18, 57);"> </font><font style="color:rgb(13, 18, 57);">Because it needs to leverage the</font><font style="color:rgb(13, 18, 57);"> </font>**<font style="color:rgb(13, 18, 57);">Primacy Effect</font>**<font style="color:rgb(13, 18, 57);">. System instructions, role definitions, and core rules are global and must be firmly remembered by the model as the foundation of the entire conversation. Placing them at the beginning ensures they receive the highest attention weights.</font>
+ **<font style="color:rgb(13, 18, 57);">Why must User Input be at the end?</font>**<font style="color:rgb(13, 18, 57);"> Because it needs to leverage the </font>**<font style="color:rgb(13, 18, 57);">Recency Effect</font>**<font style="color:rgb(13, 18, 57);">. The user's specific question is the instruction that the model needs to respond to immediately. Placing it at the end ensures it becomes the focus of the model's attention, directly driving the generation process.</font>

#### Data Layering
**Purpose:**

Use different vector dimensions and different index types for different data, layering the underlying data, achieving the most reasonable and efficient utilization through **sub-table storage + intelligent routing**. For example, main table 1536 dimensions, working memory table 512 dimensions, archive table 256 dimensions



**Core Implementation Logic:**

Use metadata (JSON) in database table fields to route and distribute based on different filters



**Prerequisites:**

Sub-table data and main table data **have no intersection**. For example, sub-table is shared memory, main table is private memory. When querying with type=XX, only one table will be queried



**Routing Flow Diagram**

![](https://intranetproxy.alipay.com/skylark/lark/__mermaid_v3/e14e73aaed11c6f4f856f93529ea1677.svg)

#### OB Graph Implementation
Graph databases are generally designed to solve <font style="color:rgb(13, 18, 57);">querying large amounts of data with</font>**<font style="color:rgb(13, 18, 57);"> complex relationships</font>**, such as blockchain and decentralized applications. However, in RAG and memory retrieval environments, multi-hop requirements are often limited, and the amount of relationship data between entities is predictable. Therefore, relational databases can achieve the same purpose.



**Table Structure**

```plain
graph_entities【Entities】
├── id (Primary Key)
├── name (Entity Name)
├── entity_type (Entity Type)
├── embedding (Vector Embedding)
├── created_at (Created At)
└── updated_at (Updated At)


graph_relationships【Relationships】
├── id (Primary Key)
├── source_entity_id (Source Entity ID)
├── destination_entity_id (Destination Entity ID)
├── relationship_type (Relationship Type)
├── user_id (User ID)
├── agent_id (Agent ID)
├── run_id (Run ID)
├── created_at (Created At)
└── updated_at (Updated At)
```

**Multi-hop Search**

```plsql
-- Hop 1
SELECT
    ...
FROM
    (
    SELECT
        ...
    FROM {self.relationship_table}
    WHERE
        source_entity_id IN ('alice_uuid', 'bob_uuid')  -- Seed entities
        AND r.user_id = 'user123'
    ORDER BY mentions DESC, created_at DESC
    LIMIT 1000
) AS r
JOIN {self.entity_table} e1 ON r.source_entity_id = e1.id
JOIN {self.entity_table} e2 ON r.destination_entity_id = e2.id;

-- Hop 2
-- Structure is identical, only parameters differ
SELECT
    ...
FROM
    (
    SELECT
        ...
    FROM {self.relationship_table}
    WHERE
        source_entity_id IN ('openai_uuid', 'sf_uuid', 'charlie_uuid', ...)  -- Hop 1's destination_id
        AND r.user_id = 'user123'
    ORDER BY mentions DESC, created_at DESC
    LIMIT 1000
) AS r
JOIN {self.entity_table} e1 ON r.source_entity_id = e1.id
JOIN {self.entity_table} e2 ON r.destination_entity_id = e2.id;

-- Hop 3
-- Structure is identical, only parameters differ
....
```

![](https://intranetproxy.alipay.com/skylark/lark/__mermaid_v3/3597d193def6ef4e703bb0805a9c2f25.svg)

**Example**

```plain
Initial: current_sources = ['alice_uuid', 'bob_uuid']

┌─────────────────────────────────────┐
│  Hop 1: SQL-1                       	│
│  IN ('alice_uuid', 'bob_uuid')      │
│  Returns: 30 relationships         	│
└──────────┬──────────────────────────┘
           │
           ├─ Application layer loop prevention filtering
           ├─ Accumulated results: 30
           ├─ Check: 30 < 100, continue
           │
           └─ Extract destination_id as next hop starting point
              current_sources = ['openai_uuid', 'sf_uuid', ...]

┌─────────────────────────────────────┐
│  Hop 2: SQL-2                       	│
│  IN ('openai_uuid', 'sf_uuid', ...) │
│  Returns: 45 relationships          	│
└──────────┬──────────────────────────┘
           │
           ├─ Application layer loop prevention filtering (filter out 5 duplicate edges)
           ├─ Accumulated results: 30 + 40 = 70
           ├─ Check: 70 < 100, continue
           │
           └─ Extract destination_id as next hop starting point
              current_sources = ['sam_uuid', 'ca_uuid', ...]

┌─────────────────────────────────────┐
│  Hop 3: SQL-3                       	│
│  IN ('sam_uuid', 'ca_uuid', ...)    │
│  Returns: 40 relationships          	│
└──────────┬──────────────────────────┘
           │
           ├─ Application layer loop prevention filtering (filter out 5 duplicate edges)
           ├─ Accumulated results: 70 + 35 = 105
           ├─ Check: 105 >= 100 ✅ Limit satisfied!
           │
           └─ Return all_results[:100]
```

### Features
#### <font style="color:rgb(62, 62, 62);">Intelligent Memory Management</font>
Through time decay, importance assessment, and spaced repetition mechanisms, like human memory, implementing adaptive forgetting mechanisms to prevent information overload.

<font style="color:rgb(62, 62, 62);"></font>

<font style="color:rgb(62, 62, 62);">Algorithm Foundation:</font>

![image](https://intranetproxy.alipay.com/skylark/lark/__latex/da72504d1dbc9ecc988ebfa0d1a7b4eb.svg)

Where:

+ R: Memory Retention Rate, range [0, 1]
+ t: Time elapsed, unit: hours
+ S: Memory Strength, determined by decay_rate<font style="color:rgb(62, 62, 62);"></font>

**Memory Addition Process**

![](https://intranetproxy.alipay.com/skylark/lark/__mermaid_v3/46230a4bf8a3f429e2333c60086496b6.svg)



**Memory Access Process**

![](https://intranetproxy.alipay.com/skylark/lark/__mermaid_v3/e1bd1899233317918c812351cb3ede8e.svg)



Memory Metadata Structure

```json
{
    "intelligence": {
        "importance_score": 0.85,           # Importance score
        "memory_type": "long_term",         # Memory type
        "initial_retention": 0.85,          # Initial retention rate
        "decay_rate": 0.1,                  # Decay rate
        "current_retention": 0.72,          # Current retention rate
        "next_review": "2024-01-02T10:00:00",  # Next review time, review will "reset" the forgetting curve, restarting decay from the review time point
        "review_schedule": [...],           # Review schedule
        "last_reviewed": "2024-01-01T10:00:00",  # Last reviewed time
        "review_count": 2,                  # Review count
        "access_count": 5,                  # Access count
        "reinforcement_factor": 0.3         # Reinforcement factor, adjusts retention rate
    },
    "memory_management": {
        "should_promote": False,            # Whether should promote
        "should_forget": False,             # Whether should forget
        "should_archive": False,            # Whether should archive
        "is_active": True                   # Whether active
    },
    "created_at": "2024-01-01T10:00:00",   # Created at
    "updated_at": "2024-01-01T15:00:00"    # Updated at
}
```

# <font style="color:rgb(62, 62, 62);">References</font>
1. [《The Rise and Potential of Large Language Model Based Agents: A Survey》](https://arxiv.org/abs/2309.07864)
2. [《Lost in the Middle: How Language Models Use Long Contexts》](https://arxiv.org/abs/2307.03172)

