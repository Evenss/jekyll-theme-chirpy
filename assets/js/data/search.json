[
  
  {
    "title": "Ai Memory",
    "url": "/posts/ai-memory/",
    "categories": "",
    "tags": "",
    "date": "2025-11-18 00:00:00 +0800",
    "content": "Introduction to Memory Agent Architecture   (From The Rise and Potential of Large Language Model Based Agents: A Survey):     Perception: Information input.   Brain-Decision Making: Autonomous decision-making and planning, executing more complex tasks.   Brain-Memory &amp; Knowledge: Memory capability, storing Agent’s knowledge and skills.   Action: Interacting with the external world, enabling Agents to autonomously complete more complex tasks through actions and perception.   The Role of Memory for Agents         Enable Agents to have continuous learning capabilities                     Summarize experiences from past interactions and learn from mistakes to improve task performance.                       Enable Agents to maintain conversational coherence and action consistency:                     Possess longer-range context management capabilities, maintaining consistent context in long conversations to ensure coherence.                       Avoid establishing facts that contradict previous ones, maintaining action consistency.                       Enable Agents to provide personalized services and user experiences        Memory VS RAG    AI Memory is more like the model's \"personal experience\" or \"conversation history\", focusing on continuity of internal state and personalization.   RAG is like equipping the model with a \"real-time reference library\", focusing on obtaining external, authoritative, and up-to-date information to improve answer accuracy and reliability.   Simply put, you can think of AI Memory as the model \"remembering\" what it has discussed with you, while RAG is the model \"looking up information\" when answering your questions to ensure correctness. An intelligent AI assistant will likely use both capabilities simultaneously.  Differences | Feature | AI Memory | RAG (Retrieval-Augmented Generation) | | :— | :— | :— | | Core Mechanism | Influences future outputs by storing and retrieving the model's own interaction history or internal knowledge. Memory can be short-term (e.g., conversation context) or long-term (e.g., past experiences stored in vector databases). | Before generating a response, retrieves relevant information from external knowledge bases (e.g., documents, databases, web pages) and uses the retrieved content as context input to the model. | | Knowledge Source | Primarily from the model's interaction history with users or system-preset long-term memory. | From external, independent, typically structured knowledge bases. | | Primary Purpose | 1. Maintain context consistency (e.g., multi-turn conversations).2. Personalization (remembering user preferences).3. Learning and adaptation (learning from past experiences). | 1. Provide latest, accurate factual information.2.  Reduce hallucinations (generating based on retrieved real information).3. Extend model knowledge boundaries (accessing information beyond training data). | | Knowledge Updates | Memory can be dynamically updated, but requires managing memory storage, retrieval, and forgetting. Updates may affect model behavior. | Knowledge bases can be updated independently without retraining the model. Updates are decoupled from the model. | | Implementation | + Short-term memory: Directly using the model's context window.+ Long-term memory: Using vector databases to store and retrieve key information fragments. | + Retriever (e.g., vector-based similarity search).+ Generator (large language model).+ Knowledge base (document collection). | | Typical Applications | Chatbots maintaining conversation coherence, personalized assistants, AI agents requiring long-term learning. | Question-answering systems, customer service knowledge bases, fact-checking, report generation (requiring external references). |  Memory Implementation Mem0   Components         L     LM (OpenAI, Ollama, Azure OpenAI, Gemini, DeepSeek, and 18 other types)           Vector databases     (Qdrant, Chroma, Pgvector, Milvus, and 19 other types)           Graph databases     (Neo4j, Memgraph, Neptune Analytics, Kuzu)           Embedding models     (OpenAI, Azure OpenAI, Ollama, and 10 other types)      Architecture         API interface layer           Intelligent processing layer: LLM reasoning + fact extraction + conflict resolution            Fact extraction: Using custom prompts to extract key information from conversations       Conflict resolution: Intelligently determining ADD/UPDATE/DELETE/NONE operations       Memory optimization: Avoiding redundancy, keeping information up-to-date                Dual storage architecture: Vector database + Graph database           Factory layer: LLM/Embedder/VectorStore Factory      Process Main Process    Retrieval Process    Source Code Analysis # mem0's concurrent dual-path retrieval implementation with concurrent.futures.ThreadPoolExecutor() as executor:     # Path 1: Vector database semantic search     future_memories = executor.submit(self._search_vector_store, ...)      # Path 2: Graph database relationship search       future_graph_entities = executor.submit(self.graph.search, ...)      # Get results     vector_memories = future_memories.result()     # Retrieval results     graph_relations = future_graph_entities.result()  # Graph path entity relationship results   # Return separately return {     \"results\": vector_memories,    # Vector path results     \"relations\": graph_relations   # Graph path entity relationship results  }   def search(     self,     query: str,     vectors: list[float],     limit: Optional[int] = 5,     filters: Optional[dict] = None, ) -&gt; List[OutputData]:          ...          # Filter conditions     if filters:         for k, v in filters.items():             filter_conditions.append(\"payload-&gt;&gt;%s = %s\")             filter_params.extend([k, str(v)])     filter_clause = \"WHERE \" + \" AND \".join(filter_conditions) if filter_conditions else \"\"      # Execute SQL: scalar + vector     with self._get_cursor() as cur:         cur.execute(             f\"\"\"             SELECT id, vector &lt;=&gt; %s::vector AS distance, payload             FROM {self.collection_name}             {filter_clause}             ORDER BY distance             LIMIT %s             \"\"\",             (vectors, *filter_params, limit),         )          results = cur.fetchall()     ...   def search(self, query, filters, limit=100):     ...      # Step 1: LLM extracts entities     entity_type_map = self._retrieve_nodes_from_data(query, filters)     # Step 2: Graph search for related entities and relationships     search_output = self._search_graph_db(node_list=list(entity_type_map.keys()), filters=filters)      ...          # Step 3: BM25 reranks search results     bm25 = BM25Okapi(search_outputs_sequence)      tokenized_query = query.split(\" \")     reranked_results = bm25.get_top_n(tokenized_query, search_outputs_sequence, n=5)      ...  # Implementation of _search_graph_db graph search def _search_graph_db(self, node_list, filters, limit=100):          # Filter conditions     ...     node_props_str = ...          for node in node_list:         n_embedding = self.embedding_model.embed(node)              # Node retrieval based on vector similarity         cypher_query = f\"\"\"         -------Match candidate nodes-------         ---self.node_label = \":`__Entity__`\" if self.config.graph_store.config.base_label else \"\"         MATCH (n {self.node_label} })         WHERE n.embedding IS NOT NULL         -------Calculate cosine similarity-------         WITH n, round(2 * vector.similarity.cosine(n.embedding, $n_embedding) - 1, 4) AS similarity         WHERE similarity &gt;= $threshold         -------Find outgoing and incoming edges-------         CALL              WITH n             MATCH (n)-[r]-&gt;(m {self.node_label} })             RETURN n.name AS source, elementId(n) AS source_id, type(r) AS relationship, elementId(r) AS relation_id, m.name AS destination, elementId(m) AS destination_id             UNION             WITH n               MATCH (n)&lt;-[r]-(m {self.node_label} })             RETURN m.name AS source, elementId(m) AS source_id, type(r) AS relationship, elementId(r) AS relation_id, n.name AS destination, elementId(n) AS destination_id                  WITH distinct source, source_id, relationship, relation_id, destination, destination_id, similarity         RETURN source, source_id, relationship, relation_id, destination, destination_id, similarity         ORDER BY similarity DESC         LIMIT $limit         \"\"\"              params = {             \"n_embedding\": n_embedding,             \"threshold\": self.threshold,             \"user_id\": filters[\"user_id\"],             \"limit\": limit,         }                  ...                  ans = self.graph.query(cypher_query, params=params)         result_relations.extend(ans)          return result_relations   PowerMem Performance Improvements Multi-path Retrieval/Hybrid Search Flow Diagram     Full-text + Vector + Scalar            Version 441 and above can directly use kernel hybrid search           Fusion coarse ranking (RRF) + Fine ranking (Rerank model)            Rerank can improve accuracy by ~6%             Source Code Analysis:  def _hybrid_search(self, query: str, vectors: List[List[float]], limit: int = 5, filters: Optional[Dict] = None,                    fusion_method: str = \"rrf\", k: int = 60):      # Expand single-path limit by 3x     candidate_limit = limit * 3 if self.reranker else limit      # Step 1: Query separately     with ThreadPoolExecutor(max_workers=2) as executor:         # Vector         vector_future = executor.submit(self._vector_search, query, vectors, candidate_limit, filters)         # Full-text         fts_future = executor.submit(self._fulltext_search, query, candidate_limit, filters)          vector_results = vector_future.result()         fts_results = fts_future.result()      # Step 2: Fuse results for coarse ranking, limit results to 3 * limit     coarse_ranked_results = self._combine_search_results(         vector_results, fts_results, candidate_limit, fusion_method, k     )           # Step 3: Fine ranking (using qwen3-rerank model), limit results to limit     if self.reranker and query and coarse_ranked_results:         final_results = self._apply_rerank(query, coarse_ranked_results, limit)         return final_results     else:         return coarse_ranked_results[:limit]  # rerank def _apply_rerank(self, query: str, candidates: List[OutputData], limit: int) -&gt; List[OutputData]:          ...      # Call reranker to get reranked indices and scores     reranked_indices = self.reranker.rerank(query, documents, top_n=limit)          ...          # Special handling: high-scoring items at head and tail, because LLMs have recency effect     if len(final_results) &gt; 1:         reordered = [None] * len(final_results)         left = 0         right = len(final_results) - 1                  for i, result in enumerate(final_results):             if i % 2 == 0:                 # Even indices go to the left side                 reordered[left] = result                 left += 1             else:                 # Odd indices go to the right side                 reordered[right] = result                 right -= 1                  final_results = reordered      return final_results   Recency Effect Source:          Paper Title: Lost in the Middle: How Language Models Use Long Contexts     Authors: Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Catasta, Percy Liang     Published: 2023     Link: https://arxiv.org/abs/2307.03172         U-shaped Performance Curve: The core finding of the paper is that when models need to retrieve information from long input texts (long contexts), their performance shows a clear \"U-shaped\" curve.  - &lt;font style=\"color:rgb(13, 18, 57);\"&gt;Information located at the&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt; beginning (Primacy) and end (Recency)&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt; of the input sequence can be most accurately recalled and used by the model.&lt;/font&gt; - &lt;font style=\"color:rgb(13, 18, 57);\"&gt;Information located in the&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt; middle&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt; of the input sequence shows significantly decreased accuracy in recall and usage, as if \"lost in the middle\".&lt;/font&gt;   Paper Conclusions     Why must System Prompt be at the front? Because it needs to leverage the Primacy Effect. System instructions, role definitions, and core rules are global and must be firmly remembered by the model as the foundation of the entire conversation. Placing them at the beginning ensures they receive the highest attention weights.   Why must User Input be at the end? Because it needs to leverage the Recency Effect. The user's specific question is the instruction that the model needs to respond to immediately. Placing it at the end ensures it becomes the focus of the model's attention, directly driving the generation process.   Data Layering Purpose:  Use different vector dimensions and different index types for different data, layering the underlying data, achieving the most reasonable and efficient utilization through sub-table storage + intelligent routing. For example, main table 1536 dimensions, working memory table 512 dimensions, archive table 256 dimensions  Core Implementation Logic:  Use metadata (JSON) in database table fields to route and distribute based on different filters  Prerequisites:  Sub-table data and main table data have no intersection. For example, sub-table is shared memory, main table is private memory. When querying with type=XX, only one table will be queried  Routing Flow Diagram    OB Graph Implementation Graph databases are generally designed to solve querying large amounts of data with complex relationships, such as blockchain and decentralized applications. However, in RAG and memory retrieval environments, multi-hop requirements are often limited, and the amount of relationship data between entities is predictable. Therefore, relational databases can achieve the same purpose.  Table Structure  graph_entities【Entities】 ├── id (Primary Key) ├── name (Entity Name) ├── entity_type (Entity Type) ├── embedding (Vector Embedding) ├── created_at (Created At) └── updated_at (Updated At)   graph_relationships【Relationships】 ├── id (Primary Key) ├── source_entity_id (Source Entity ID) ├── destination_entity_id (Destination Entity ID) ├── relationship_type (Relationship Type) ├── user_id (User ID) ├── agent_id (Agent ID) ├── run_id (Run ID) ├── created_at (Created At) └── updated_at (Updated At)   Multi-hop Search  -- Hop 1 SELECT     ... FROM     (     SELECT         ...     FROM {self.relationship_table}     WHERE         source_entity_id IN ('alice_uuid', 'bob_uuid')  -- Seed entities         AND r.user_id = 'user123'     ORDER BY mentions DESC, created_at DESC     LIMIT 1000 ) AS r JOIN {self.entity_table} e1 ON r.source_entity_id = e1.id JOIN {self.entity_table} e2 ON r.destination_entity_id = e2.id;  -- Hop 2 -- Structure is identical, only parameters differ SELECT     ... FROM     (     SELECT         ...     FROM {self.relationship_table}     WHERE         source_entity_id IN ('openai_uuid', 'sf_uuid', 'charlie_uuid', ...)  -- Hop 1's destination_id         AND r.user_id = 'user123'     ORDER BY mentions DESC, created_at DESC     LIMIT 1000 ) AS r JOIN {self.entity_table} e1 ON r.source_entity_id = e1.id JOIN {self.entity_table} e2 ON r.destination_entity_id = e2.id;  -- Hop 3 -- Structure is identical, only parameters differ ....     Example  Initial: current_sources = ['alice_uuid', 'bob_uuid']  ┌─────────────────────────────────────┐ │  Hop 1: SQL-1                       \t│ │  IN ('alice_uuid', 'bob_uuid')      │ │  Returns: 30 relationships         \t│ └──────────┬──────────────────────────┘            │            ├─ Application layer loop prevention filtering            ├─ Accumulated results: 30            ├─ Check: 30 &lt; 100, continue            │            └─ Extract destination_id as next hop starting point               current_sources = ['openai_uuid', 'sf_uuid', ...]  ┌─────────────────────────────────────┐ │  Hop 2: SQL-2                       \t│ │  IN ('openai_uuid', 'sf_uuid', ...) │ │  Returns: 45 relationships          \t│ └──────────┬──────────────────────────┘            │            ├─ Application layer loop prevention filtering (filter out 5 duplicate edges)            ├─ Accumulated results: 30 + 40 = 70            ├─ Check: 70 &lt; 100, continue            │            └─ Extract destination_id as next hop starting point               current_sources = ['sam_uuid', 'ca_uuid', ...]  ┌─────────────────────────────────────┐ │  Hop 3: SQL-3                       \t│ │  IN ('sam_uuid', 'ca_uuid', ...)    │ │  Returns: 40 relationships          \t│ └──────────┬──────────────────────────┘            │            ├─ Application layer loop prevention filtering (filter out 5 duplicate edges)            ├─ Accumulated results: 70 + 35 = 105            ├─ Check: 105 &gt;= 100 ✅ Limit satisfied!            │            └─ Return all_results[:100]   Features Intelligent Memory Management Through time decay, importance assessment, and spaced repetition mechanisms, like human memory, implementing adaptive forgetting mechanisms to prevent information overload.    Algorithm Foundation:    Where:     R: Memory Retention Rate, range [0, 1]   t: Time elapsed, unit: hours   S: Memory Strength, determined by decay_rate   Memory Addition Process    Memory Access Process    Memory Metadata Structure  {     \"intelligence\": {         \"importance_score\": 0.85,           # Importance score         \"memory_type\": \"long_term\",         # Memory type         \"initial_retention\": 0.85,          # Initial retention rate         \"decay_rate\": 0.1,                  # Decay rate         \"current_retention\": 0.72,          # Current retention rate         \"next_review\": \"2024-01-02T10:00:00\",  # Next review time, review will \"reset\" the forgetting curve, restarting decay from the review time point         \"review_schedule\": [...],           # Review schedule         \"last_reviewed\": \"2024-01-01T10:00:00\",  # Last reviewed time         \"review_count\": 2,                  # Review count         \"access_count\": 5,                  # Access count         \"reinforcement_factor\": 0.3         # Reinforcement factor, adjusts retention rate     },     \"memory_management\": {         \"should_promote\": False,            # Whether should promote         \"should_forget\": False,             # Whether should forget         \"should_archive\": False,            # Whether should archive         \"is_active\": True                   # Whether active     },     \"created_at\": \"2024-01-01T10:00:00\",   # Created at     \"updated_at\": \"2024-01-01T15:00:00\"    # Updated at }   References    《The Rise and Potential of Large Language Model Based Agents: A Survey》   《Lost in the Middle: How Language Models Use Long Contexts》"
  }
  
]

